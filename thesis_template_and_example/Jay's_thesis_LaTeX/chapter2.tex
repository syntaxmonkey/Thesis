\chapter{Previous Work}\label{chap:pw}

\section{Introduction}

This chapter describes previous research trends and solutions related to the three topics we discuss in this thesis: Artistic Stylization, Image Upsampling and Image Warping. We review these previous publications here as it will helps us to better recognize and understand the problems, identify and avoid known pitfalls, design and improve our algorithms, and compare and evaluate our results. 

Image upsampling is the operation that constructs a higher-resolution image from lower-resolution input~\cite{Shan:fiu}. Section~\ref{bg:upsampling} lays out previous efforts on polynomial interpolation-based upsampling, example-based upsampling, and the techniques that use variations of low-resolution input to facilitate upsampling. We also discussed processes we adopted to gather edge or texture information. 

Artistic stylization refers to the techniques capable of transforming 2D footage into synthetic artwork~\cite{NPRbook}. Section~\ref{bg:Artistic Stylization} presents previous approaches that produces various artistic styles. We adopted the watercolor filter by Bousseau et al.~\cite{Bousseau:2007}. in our image warping research.

Section~\ref{bg:imagewarping} describes previous literature that is relevant to our painterly effect and image warping research. We discussed painterly effects with a special emphasis on extreme effects that are similar to our proposed effect. We also included literature about image segmentation, since it is one important component in our proposed pipeline.

\newpage
\section{Image Upsampling } \label{bg:upsampling}

Image upsampling is the operation which estimates a fine, high-resolution image from a coarse, low-resolution input~\cite{Shan:fiu}. High-resolution means high pixel density within an image. Higher-resolution images offer finer details, which is of great importance in applications such as medical imaging, satellite imaging, military imaging, underwater imaging, and HDTV~\cite{book:superRes}. 

The direct solution to achieve high resolution is to reduce pixel size in sensor manufacturing, that is, to increase the number of pixels per unit area. However, the drawback is that the amount of available light for each pixel is decreased. Raising sensor density also greatly increases the hardware expense. Therefore, to achieve high-resolution image, the most feasible solution is to integrate hardware capabilities with software support, i.e., image upsampling techniques~\cite{book:superRes}. This section discusses trends and processes in image upsampling techniques.

\subsubsection{Polynomial image interpolation}
Image upsampling increases pixel count, which means the pixels from low-resolution input are only sparsely placed on the upsampled image plane. The value of intermediate pixels are unknown. The process of estimating the intermediate pixels between known pixels is called image interpolation. 

The traditional view of interpolation is that the interpolation can be represented as an arbitrary continuous function, which is a discrete sum of weighted or shifted synthesis functions. We use the terminology of polynomial, because the interpolation can be performed using polynomials of a spacing parameter~\cite{book:superRes}. Given the spacings between unknown pixels and known, regularly sampled pixel data, we can use polynomial functions to estimate unknown pixel values. Nearest-neighbour interpolation, bilinear interpolation~\cite{Unser:spline}, and bicubic interpolation~\cite{bicubic} are popular polynomial interpolations. They are popular due to their simplicity. However, they usually introduce severe blocking, aliasing, or blurring artifacts. 

There exist many adaptive, improved polynomial interpolation approaches~\cite{warpeddistance,regInterpolation,adaptiveInter}. They focus on either improving visual quality (alleviating blurring artifacts or preserving edges) or reducing computational cost of the interpolation algorithm. They produce acceptable results when the upsampling scale is small. However, as the upsampling scale increases, they are unlikely to be capable of constructing sharp results, due to the sparser known-pixel distribution. 

\subsubsection{Example-based upsampling}\label{bg:examplebasedUpsampling}
Example-based upsampling creates or adds high-resolution details that are not present in the original image, with the help or guidance from other images. The ``other images'' here refer images found in image databases, as well as downsampled or processed original image and original image segments. Example-based upsampling has been a popular trend over the past decade. It is powerful because the known pixel data is no longer limited to the pixel data of the input. We also have access to pixels from image databases. We can use them to help estimate the unknown intermediate pixels. 

Freeman et al. developed example-based super resolution~\cite{Freeman:pct}, a fast and simple, one-pass, training-based algorithm for creating plausible high-frequency details in enlarged images. They divide blurry polynomial interpolated images into patches (small windows centered at each pixel). They find high-resolution details in image database that matches the patches. Then they replace the patches with the matching high-resolution details. Kim and Kwon proposed an improved example-based super-resolution framework~\cite{Kim:singlImg}. Instead of finding the best match in the image database, they find multiple candidates. Then they combine the candidates based on their estimated confidence. Glasner et al. proposed an example-based approach that uses the original image segments as image database~\cite{Glasner:singlSup}. They observed that patches in a natural image tend to redundantly recur many times. Their approach identifies the recurrence of patches in a single image, then uses these patches as database to achieve example-based super-resolution. Based on the patch-based model of Freeman et.al.~\cite{Freeman:pct}, Hacohen et al.~\cite{Fattal:Hallucination} proposed an improved method that consider textured areas as large segments instead of patches. They discovered that Freeman\textquotesingle s patch-based model could only find locally best match, but lacked global consistency. By segmenting input image into large, similar-textured regions, they are able to find globally consistent matches in the image database. Thus, their algorithm introduces fine-scale, consistent details to large textured regions.

Overall, example-based approaches have the advantage of recreating fine details. They learn information from segments of the original image, and use this information to find higher-resolution textures or image segments in image database. However, these matching image segments, no matter how closely they resemble the example, are not the same as the example. Thus, they achieve higher resolution with fine details by sacrificing the integrity of the original input. Moreover, to achieve close resemblance, example-based approaches usually involve managing, categorizing and searching through a vast collection of images. These tasks add difficulty to the image upsampling problem. The research in this thesis avoids using example-based approaches. The hope is that we can achieve high-resolution images without or with very limited help from other images. 

\subsubsection{Use lower-resolution as guidance}\label{bg:otherApproaches}

There are several approaches that use processed or lower-resolution input to guide the upsampling. Fattal proposed an edge statistics approach to achieve sharp upsampled images~\cite{FAT:EdgeStatistics}. He points out that there exists a unique dependency between image derivatives at different resolutions. Pixel differences at higher resolutions depend on their distance from an edge, the spatial distribution of that edge and the total intensity jump across it. Such information are called edge statistics, and can be collected and estimated in low-resolution. Fattal proposed methods to predict intensity differences in the upsampled image given the edge parameters observed at the low-resolution input. The edges retrieved are sharp. The blocky artifacts coming from upsampling are minimal. 

Shan et al.~\cite{ Shan:fiu} proposed a fast upsampling method that uses deconvolution~\cite{Shan:2008} result of upsampled image as feedback to enhance upsampling process. By using a Gaussian kernel, they introduced an iterative de-convolution and re-convolution scheme that is called feedback control loop. They gather information of the pixels discarded by the feedback control loop, and introduce these pixels to upsampled image plane to enhance details. Their approach simulates an inversion of the image formation process and produces a clear, finer-resolution image based on a coarse-level image. Essentially, their approach is observing the difference between the input image and processed input image, and using the difference to help achieve super-resolution. 

Kopf et al. proposed a joint bilateral upsampling\cite{kopf:JBilateral} that improves a high-resolution input image by cross-filtering with its filtered low-resolution solution. The idea of joint bilateral filter is that range filter is applied to a second guidance image. In the context of image upsampling, this guidance image is usually the downsampled (or the original) image. We adopted the cross filtering idea in our bilateral roundup filter and cumulative range geodesic filter. Applying these filters directly on the upsampled image will result a thick band-shaped mask along sharp edges. It is not ideal because these pixels are interpolated and smooth: they destroyed the gradient intensity along sharp edges. However, if we cross-filter with the original image, we will not include these interpolated pixels in the mask. Thus, the upsampled edges are expected to be sharper.



\subsubsection{Texture measurement and enrichment}

Both image upsampling and image abstraction processes can destroy fine details or introduce smooth, dull regions. Though we do not focus on maintaining the photorealistic look, we want the upsampled images to include small-scale details, by either replicating the texture from the input image, or borrowing textures from other images~\cite{Jordan}. This requires us to distinguish highly-textured areas (e.g. grass, sand) from smooth areas (e.g. sky). 

We define textureness as how frequently the color intensity changes around the pixel neighbourhood. Bae et al.~\cite{Bae:2006} proposed a textureness measurement that we find quite useful. Their approach starts by high-pass filtering the frequency domain of the input. Then they build power maps by finding local average obtained from low-pass filtering of the frequency magnitude. Bae\textquotesingle s approach enable us to identify textured regions of the original image automatically. These textured regions are likely to be destroyed by the upsampling and interpolation. Having Bae\textquotesingle s textureness map, we can add another term to our bilateral filter or cumulative range geodesic filter, which helps us to sample pixels differently on highly textured areas. Alternatively, we can also introduce textures specifically for those areas after upsampling.

Xu et al. proposed an image smoothing approach using L0 gradient minimization~\cite{Xu:L0}. Their work sharpens major edges and eliminates some degree of low-amplitude structures. They globally control how many non-zero gradients are permitted in order to approximate prominent structure in a sparsity-control manner. In contrast to Bae\textquotesingle s approach, Xu\textquotesingle s L0 gradient minimization discards local high-frequency information, but preserves globally significant structures. Thus, L0 gradient minimization helps us distinguish globally important edges. Also, combined with an ordinary gradient magnitude map, L0 gradient minimization lets us identify discarded textures. Similar to Bae\textquotesingle s approach, we can potentially use this textureness information to guide the sampling process of various filters we applied.
 
Overall, previous image upsampling literature pursues a photographic style that precisely respects the low-resolution input. They want to preserve salient features as well as estimating or reconstructing fine details. Our research shares the goal of preserving sharp, salient features. However, we also allow the result images to be stylized. We hope that by introducing a stylized look, losing or changing fine details would become tolerable; and eventually we can produce upsampled images that are aesthetically pleasing.

\section{Artistic Stylization}\label{bg:Artistic Stylization}

Artistic stylization refers to the techniques capable of transforming 2D footage into synthetic artwork~\cite{NPRbook}. There has been a vast amount of publications in this field, and they diversify for different styles and aesthetic values. This section lays out the literature that aims for styles or effects that are comparable to our approach. We also describe operations, algorithms, and processes that we adopted or found inspiring.

\subsubsection{Stroke-Based rendering}
Stroke-based rending is the process of synthesizing artwork by compositing rendering marks (such as lines, brush strokes, or even larger primitives such as tiles) upon a digital canvas~\cite{NPRbook}. 

The stroke-based rendering paradigm was proposed in Haeberli\textquotesingle s seminal work on user-assisted image stylization~\cite{Hae:stroke}. Haeberli\textquotesingle s approach creates abstract images using an ordered collection of paint strokes. Given photographic images as input, by controlling the color, shape, size, and orientation of individual brush strokes, Haeberli\textquotesingle s approach can create various impressionistic paintings. Inspired by his work, the general pipeline of our research is similar. We take photographic images as input; apply various filtering, color manipulation, pixel migration techniques; and create various types of stylized images as output. Moreover, many operations and processes adopted by our research are inspired by the techniques mentioned in Haeberli\textquotesingle s paper. For instance, his pushing edges process is one of the earlier edge exaggeration processes we have investigated; our experiments on residual (difference between before and after filtering) calculation are similar to Haeberli\textquotesingle s color richness enhancement technique.

Following Haeberli\textquotesingle s work, stroke-based rendering became a popular trend for painterly style rendering. Litwinowicz proposed a stroke-based technique for impressionist effect~\cite{Litwinowicz}. His approach respects object boundaries by clipping the paint strokes. He also orients, moves, adds, or deletes paint strokes to match the image content. Hertzman proposed another method that creates impressionist images with a series of spline brush strokes~\cite{Hertzmann:1998}. Brush strokes match the colors of the image source, and several layers of progressively smaller brush strokes are applied so that the visual emphasis in the painting corresponds roughly to the spatial energy present in the source image. The technique for painting long, curved brush strokes aligned with normals of image gradients is inspiring. We, too, want to recognize object boundaries or edge orientations and use that information to guide our filtering processes.

\subsubsection{Filtering-based Rendering}
Filtering is a common yet essential part of many signal processing systems, including image processing. Various filtering techniques serve quite different purposes, such as image blurring or sharpening, edge detection and exaggeration, noise introduction or removal, etc. By combining and varying these filtering techniques, we can achieve a wide range of artistic styles. 

Winnemoller proposed XDoG, an extended difference-of-Gaussians filtering process that can generate various stylized images~\cite{Winnemoller:2011}. The filter can control edge-detection sensitivity, thus change the tone-mapping response of the operator. It can also adjust the thresholding so that a soft ramp between the edge and non-edge values can be created. Overall, by controlling the parameters, XDoG simulates styles like natural-media style, ghosting, speed-lines, negative edges, thresholding, hatching, etc. Our research does not directly use XDoG filter, but we find the idea of thresholding and using edge-detection sensitivity to guide mask generation inspiring. 

Kass and Solomon~\cite{HistogramFilter} proposed a series of filters based on a smoothed histogram analysis of pixel neighbourhood. The traditional histogram sorts data in to a collection of bins. Kass and Solomon proposed a smoothed histogram by applying a Gaussian filter to the traditional histogram. Based on this smoothed histogram and its derivative, they proposed methods to identify the number of modes, the mode values, the mode widths and the percentage of the population contained within each mode. With this information, they present a series of filters and operations: a closest-mode filter for edge-preserving noise reduction; a median filter for extreme-noise reduction; a percentile filter that resembles morphological operators; a dominant-mode filter for edge sharpening; and several detail enhancement techniques based on dominant-mode filter. Kass and Solomon\textquotesingle s work shares some goals with ours: edge sharpening, detail enhancing. However they focus on the direction of mask data processing rather than our direction of mask data gathering. Compared to their work, the way we process our mask data is rather primitive: we simply just use the average color as output. They show us the histogram based mask data analysis, and how to produce different results purely from mask data processing. We would like to explore more variations of mask data analysis in future work. 

Mould introduced cumulative range geodesic filtering, a variant of geodesic filtering~\cite{David:geo, Mould2}. It preserves locally strongest edges, which lead to the preservation of not only large scale edges, but also middle to small scale structures depending on the surrounding context. The filtering process involves growing masks in a priority-based cumulative way, such that the distance in the image plane is lengthened proportional to the color distance. Therefore, irregular silhouettes can be preserved by irregular shaped masks. Though the main purpose of Mould\textquotesingle s paper is image abstraction, this research shares similar goal, which is edge sharpening. Image upsampling and interpolation processes naturally introduce blur edges, which can be sharpened by cumulative range geodesic filtering. More importantly, irregular-shaped masks enable us to gather pixel neighbourhood information in a unique, local-structure-respectful way. 

Bilateral filtering, introduced by Tomasi et al.~\cite{Tomasi:bilateral}, is an edge-preserving, image-smoothing filtering technique. It combines colors based on both their geometric closeness and the photometric similarity. The filter weighs mask samples by combining their color-space distances and Euclidean distances. Thus, the pixels that are close in both color and image plane distance will contribute more to the output of the mask. The technique of bilateral filtering is simple, but the idea of combining multiple meaningful terms into one filter is powerful. Bilateral filtering is one of the methods that we use to collect pixel neighbourhood data.

Based on traditional bilateral filtering, we evolved many variations to fit our upsampling purposes. In order to avoid using interpolated colors (which cause blurriness), we take our mask data directly from the original image. To avoid outliers and better preserve texture, we sort the mask pixels by their bilateral distance, and only take a portion of the best samples to calculate our result. To further sharpen edges, we cross-filter with the original image for several passes. All these variations and investigations are enabled by bilateral filtering. It is one of the most meaningful and powerful ways of gathering mask data.

\subsubsection{Watercolor Effect}
The physical processes behind watercolor painting and oil painting are very different. For watercolor painting, water-soluble pigments of various degrees of dryness and colors are applied onto the paper through hairy brushes. As the water flows on the surface of the paper, the pigment is transported, diluted and eventually deposited on the surface of the paper~\cite{NPRbook}. There are many stroke-based techniques to simulate oil paintings, whereas state-of-the-art watercolor effects are achieved by filtering or physics simulation. In order to distinguish ourselves from stroke-based rendering effects, we adopted variations of filtering-based watercolor effects. 

Bousseau et al.~\cite{Bousseau:2007} used morphological operators to achieve watercolor style. Basic morphological operators are dilation and erosion, which involve changing pixel color to the maximum intensity or minimum intensity color of its neighbourhood respectively. The dilation spreads the light features of the image whereas the erosion spreads the dark features. The region where morphological operators sample pixel neighbourhood data is called structuring element. The shape of the structuring element defines the shape of the filtered objects. Morphological opening is defined as a sequence of erosion followed by one dilation, whereas morphological closing is a sequence of dilation followed by one erosion. Opening operator removes light features of the image, whereas closing removes dark features. Bousseau et al. introduced a filter with the sequence of one closing operation followed by one opening operation. This filter removes both dark and light features and achieves an effect that resembles watercolor painting.

In addition to morphological operations, Bousseau et al. proposed a refined pipeline to achieve better watercolor effect~\cite{Bousseau:2006}. The pipeline separates the processes into two stages: abstraction stage and effect stage. To achieve uniform color regions, the abstraction stage segments the image and applies morphological operators. Then the effect stage adds dry-brush or paper texture, then applies edge darkening filter. Our image warping technique has a similar pipeline. We also separate our processes to two stages. The warping stage involves segmenting the image, migrating the pixels and interpolating colors. The output of this stage is a warped image. The second stage is applying selected filters. Specifically for our experiments, a morphological watercolor filter by Bousseau et al.~\cite{Bousseau:2007} is applied.

Curtis et al.~\cite{curtis} proposed another approach to achieve various artistic effects of watercolor. What is most interesting and inspired is they use a shallow-water fluid simulation to calculate how color pigments or pigments streams would travel through the paper. Their work is one of the rare cases where physics-based simulations are used for non-photorealistic rendering.  Our research uses mass-spring simulation to migrate pixels in original image plane or upsampled image plane. Though mass-spring system is not as complex as fluid simulation, they both fall into the category of physics.

\section{Image Warping for Painterly Effect}\label{bg:imagewarping}

\subsubsection{Overview of painterly rendering}
Painterly rendering has been a major topic in the non-photorealistic rendering literature. Broadly speaking, synthetic painted images can be created in three ways: through digital artists exercising synthetic painting tools~\cite{impasto,moxi}; through paint primitives being distributed according to the details of a geometric scene~\cite{meier}; or through image-space operations over an input image, usually a photograph, e.g., distributing strokes that match local image properties~\cite{Hertzmann:1998}. We are mainly interested in the last of these approaches.

Among the most-studied forms of painterly rendering is portraiture, with specialized techniques to draw human faces. Perhaps the most effective method is that of Zhou and Zhu~\cite{facepainting}, who use active shape models to obtain an estimate of facial feature locations, and then learn a mapping from user-drawn strokes to the face geometry. By using a large database of hand-painted portaits, their mapping is made to be fairly reliable; novel portraits can then be made from photos by estimating the face structure and drawing from the stroke placement database. This example-based system produces reasonably realistic drawings, albeit abstracted owing to the small number of strokes. An earlier method that is somewhat closer to our intent is provided by Gooch et al.~\cite{caricature}, who automatically estimate facial feature locations in structured photographs, then distort the face shapes to create caricatures. This work is effective but by its nature restricted to portraits with empty backgrounds. Also, the proposed rendering is a monochrome pen-and-ink style, not a painterly rendering.

In general, semantically meaningful image manipulations are difficult to achieve automatically. Two standard workarounds are either to employ user-provided semantics~\cite{sisley} or to restrict the subject matter and layout of the input images, as in the case of portraiture~\cite{caricature}. In the image warping section of our thesis, we hope to create painterly versions of arbitrary photographs, with no limitations on the layout or content. While we do not achieve the quality of results that can be attained by specialized methods applied within their target domains, this is a necessary trade-off for the robustness we gain instead. 

Painterly rendering from photographs suffers from the fixed perspective of the lens. In the case of object-space rendering, the perspective can be altered as the scene is projected onto the image plane, either through interpolating multiple linear perspectives~\cite{freshperspective} or potentially by performing an entirely nonlinear projection~\cite{Brosz,ryan}. However, such techniques depend on having a full scene description; image-space techniques are more widely applicable.

Abstraction in non-photorealistic rendering falls generally into two categories. First, a basic abstraction can be achieved using a filter to remove details: various linear and nonlinear filters have been proposed for this purpose, and we consider stroke-based rendering a special case of such filters, where an image\textquotesingle s pixel colors are replaced by the stroke colors.  Second, more deliberate shape abstraction is possible using a higher-level representation of the image; the most common approach here is to create an initial segmentation of the image and then simplify the resulting segments~\cite{glass,decarlo,colorsketch}. We are less interested in purely abstracting the image by removing detail than in reducing its faithfulness to the original photograph; we prefer our final image to be quite detailed in spite of the abstraction process.

\subsubsection{Extreme stylizations}
Quite extreme stylizations are possible, ranging from somewhat representational forms as in cubism~\cite{cubism}, to quite abstract forms as seen in the “arty shapes” of Song et al.~\cite{artyshapes}. Our image warping technique can often produce quite extreme results, especially when the input has strong salient features, like human portraits, still lives, or cityscapes.

The cubist rendering system proposed by Collomosse and Hall~\cite{cubism} has some stages in common with our approach. Taking multiple images as input, this system detects salient image elements, distorts them through transforming a super-quadric fit to the element perimeter to a different super-quadric, and then fuses multiple elements into a single image to produce a multi-perspective cubist style output. A customized painterly filter helps with the final fusion. Our pipeline uses a single image and attempts to produce a more conventionally representational output image rather than the cubist-style renderings sought by Collomosse and Hall.

Arty shapes by Song et al.~\cite{artyshapes} produces extremely abstract images. They fit each region within a segmented photograph with a variety of simple shapes, such as circles, triangles, squares, superellipses. The system automatically chooses the shape that best represents the segmented region, and several layers of shapes are oriented and scaled to fit the region. Their results exhibit very high level of abstraction. With all the small-scale or mid-scale details removed, the objects in their results look like different colored and shaped cobblestones placed together. Though high level of abstraction is rather interesting, we want to find a better balance between abstraction and photorealism. We plan to exaggerate the warping along object boundaries as much as possible, while being faithful to the input on textured areas. 

Overall, the method that is closest to ours is ``Sisley'' the abstract painter”, which is a semi-automatic approach for highly stylized painterly filtering~\cite{sisley}. The Sisley method involves segmenting the image and optionally assigning semantic labels and abstraction levels to nodes in a segmentation hierarchy; strokes are then distributed over the image plane so as to convey the image content with the appropriate abstraction level. Stroke colors and stroke geometry are perturbed, potentially dramatically, in order to achieve a high level of abstraction. This method produces quite convincing painterly images in an expressionist style. We seek a more representational style, possibly with exaggerated colors as Sisley used; our underlying process is also quite different. The Sisley method uses shape abstraction to simplify the segments, and then relies on independent perturbations of the stroke geometries to produce further distortion. Even at low levels of abstraction, details of the image are lost. Our approach separates the image warping from the painterly rendering; while we recommend applying a rendering technique such that the resulting image has a painterly appearance, most of the effect is due to the initial automatic warping and not to the painterly filter, which in any case need not be stroke-based.

\subsubsection{Image segmentation}

As mentioned above, many image stylization effects rely on segmenting the input image into regions. Segmenting an image means dividing the image into many continuous areas. The segments are distinct from each other in some way, often based on color, texture content, object boundaries, or otherwise. Individual segments are often meaningless, but meaning emerges from the overall arrangement of segments~\cite{NPRbook}. 

Comaniciu and Meer~\cite{meanshift} proposed a mean-shift approach for image segmentation. Mean-shift refers to a nonparametric procedure that identifies arbitrary shaped clusters in complex multimodal feature space. In the context of image processing, this multimodal feature space is the image plane, and the arbitrary shaped clusters are the resulting segments. Mean-shift can recover significant image features and be adapted for image segmentation. Within Comaniciu and Meer\textquotesingle s algorithm, colors are represented in the perceptually uniform color space ``Luv'', which produces region boundaries that are more meaningful for human observers. This segmentation approach is adopted by DeCarlo and Santella\textquotesingle s seminal work on segmentation driven image stylization~\cite{decarlo}. 

DeCarlo and Santella\textquotesingle s system automatically computes segmentations at different scales. A hierarchy is inferred by computing the overlap between regions at different levels, and each region is assigned a single parent, up to the top level where the entire image is a single region. This hierarchy, plus a set of image edges, is precomputed for a given image. After the hierarchy is computed, the segmentations are smoothed by low-pass filtering the segmentation boundary curves~\cite{NPRbook}. Their results exhibit a line-drawing style using bold edges and large regions of constant color. 

Achanta et al.~\cite{SLIC} proposed simple linear iterative clustering (SLIC), which outputs nearly uniform sized, high quality, compact superpixels (or segments) with a low computational expense. The algorithm performs a local clustering of pixels in the 5-D space defined by L, a, b values of the CIELAB color space and the x, y pixel coordinates. Similar to bilateral filtering, SLIC’s distance measure takes both color distance term and image plane distance term into consideration. The color distance term helps the superpixels to respect object boundaries. The image plane distance term enforces compactness and uniformity of superpixels. 

Like many other techniques that perform image-space stylization, we depend on an
initial segmentation of the image in order to preserve image features. We use 
the oversegmentation provided by SLIC. 
While the resulting segments have no semantic meaning, they
tend to respect image edges and they tend to have similar sizes to nearby
segments, a property that helps us distribute our distortion throughout the
image. Controlling the diameter of superpixels enables us to match SLIC regions with the semantic knowledge of the image. For instance, for human portraits, we may want the SLIC regions to be fairly large, so that significant facial features (eyes, eyebrows, ears, mouth, and nose) would fit in one SLIC region; whereas for nature scenes or cityscapes, we want to have smaller SLIC regions so that the high-frequency details are better preserved. Though observing and understanding the semantics of an image is not a necessary step for SLIC, we find it beneficial for our image warping processes.

We accomplish image warping using a
mass-spring system, akin to the ``pelting'' procedure of Piponi and
Borshukov~\cite{pelting}, who proposed mass-spring systems for texture
coordinate assignment. While Piponi and Borshukov sought a smooth stretching of
the textured surface, and hence used constant spring lengths, we deliberately
stretch some portions more than others to obtain warping, using spatially
varying spring lengths and spring constants to do so. The classic technique for
image warping involves thin-plate splines~\cite{tpswarp}
% from 1993
but the
smoothness of the splines creates a coherent distortion. We want our distortion
to be coherent only insofar as it modifies a single object; the warp can be
discontinuous across image edges. The mass-spring system with spring parameters
associated with SLIC regions accomplishes this aim.
